"""
Parameter Optimizer - ìë™í™”ëœ ë§¤ê°œë³€ìˆ˜ ìµœì í™”
ìœ ì „ ì•Œê³ ë¦¬ì¦˜, ê·¸ë¦¬ë“œ ì„œì¹˜, ë² ì´ì§€ì•ˆ ìµœì í™”
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any, Tuple, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import logging
import json
import asyncio
import random
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import itertools
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel

logger = logging.getLogger(__name__)

class OptimizationMethod(Enum):
    """ìµœì í™” ë°©ë²•"""
    GRID_SEARCH = "grid_search"
    GENETIC_ALGORITHM = "genetic_algorithm"
    BAYESIAN = "bayesian"
    RANDOM_SEARCH = "random_search"
    PARTICLE_SWARM = "particle_swarm"

class OptimizationObjective(Enum):
    """ìµœì í™” ëª©í‘œ"""
    SHARPE_RATIO = "sharpe_ratio"
    TOTAL_RETURN = "total_return"
    PROFIT_FACTOR = "profit_factor"
    WIN_RATE = "win_rate"
    CALMAR_RATIO = "calmar_ratio"
    SORTINO_RATIO = "sortino_ratio"

@dataclass
class ParameterRange:
    """ë§¤ê°œë³€ìˆ˜ ë²”ìœ„"""
    name: str
    min_value: float
    max_value: float
    step: Optional[float] = None
    is_integer: bool = False
    values: Optional[List[Any]] = None  # ê³ ì •ëœ ê°’ë“¤

@dataclass
class OptimizationConfig:
    """ìµœì í™” ì„¤ì •"""
    method: OptimizationMethod
    objective: OptimizationObjective
    parameter_ranges: Dict[str, ParameterRange]
    max_iterations: int = 100
    population_size: int = 50  # ìœ ì „ ì•Œê³ ë¦¬ì¦˜ìš©
    mutation_rate: float = 0.1
    crossover_rate: float = 0.8
    convergence_threshold: float = 0.001
    max_time_minutes: int = 60
    parallel_workers: int = 4
    validation_split: float = 0.3  # ê²€ì¦ìš© ë°ì´í„° ë¹„ìœ¨

@dataclass
class OptimizationResult:
    """ìµœì í™” ê²°ê³¼"""
    best_parameters: Dict[str, Any]
    best_score: float
    all_results: List[Dict[str, Any]]
    optimization_history: List[Tuple[int, float]]  # (iteration, best_score)
    convergence_achieved: bool
    total_evaluations: int
    execution_time: float
    method_used: OptimizationMethod
    objective_used: OptimizationObjective
    validation_score: Optional[float] = None
    overfitting_detected: bool = False
    timestamp: datetime = field(default_factory=datetime.now)

class ParameterOptimizer:
    """ë§¤ê°œë³€ìˆ˜ ìµœì í™”ê¸°"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.optimization_history: List[OptimizationResult] = []
        self.max_history = 50
        
        # ë² ì´ì§€ì•ˆ ìµœì í™”ìš© GP ëª¨ë¸
        self.gp_model = None
        
        logger.info(f"ğŸ”§ Parameter Optimizer initialized with {max_workers} workers")
    
    async def optimize_parameters(
        self,
        objective_function: Callable,
        config: OptimizationConfig,
        market_data: pd.DataFrame,
        validation_data: Optional[pd.DataFrame] = None
    ) -> OptimizationResult:
        """ë§¤ê°œë³€ìˆ˜ ìµœì í™” ì‹¤í–‰"""
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ¯ Starting {config.method.value} optimization for {config.objective.value}")
            
            # ë°ì´í„° ë¶„í•  (ê²€ì¦ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°)
            if validation_data is None and config.validation_split > 0:
                split_idx = int(len(market_data) * (1 - config.validation_split))
                train_data = market_data.iloc[:split_idx]
                validation_data = market_data.iloc[split_idx:]
            else:
                train_data = market_data
            
            # ìµœì í™” ë°©ë²•ì— ë”°ë¼ ì‹¤í–‰
            if config.method == OptimizationMethod.GRID_SEARCH:
                result = await self._grid_search_optimization(objective_function, train_data, config)
            elif config.method == OptimizationMethod.GENETIC_ALGORITHM:
                result = await self._genetic_algorithm_optimization(objective_function, train_data, config)
            elif config.method == OptimizationMethod.BAYESIAN:
                result = await self._bayesian_optimization(objective_function, train_data, config)
            elif config.method == OptimizationMethod.RANDOM_SEARCH:
                result = await self._random_search_optimization(objective_function, train_data, config)
            else:
                raise ValueError(f"Unsupported optimization method: {config.method}")
            
            # ê²€ì¦ ë°ì´í„°ë¡œ ê³¼ì í•© í™•ì¸
            if validation_data is not None and not validation_data.empty:
                validation_score = await self._evaluate_parameters(
                    objective_function, validation_data, result.best_parameters, config.objective
                )
                result.validation_score = validation_score
                
                # ê³¼ì í•© ê°ì§€
                score_diff = abs(result.best_score - validation_score) / result.best_score
                result.overfitting_detected = score_diff > 0.3  # 30% ì´ìƒ ì°¨ì´ë©´ ê³¼ì í•©
            
            result.execution_time = time.time() - start_time
            
            # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.optimization_history.append(result)
            if len(self.optimization_history) > self.max_history:
                self.optimization_history.pop(0)
            
            logger.info(f"âœ… Optimization completed in {result.execution_time:.2f}s: Best score {result.best_score:.4f}")
            
            return result
            
        except Exception as e:
            logger.error(f"ğŸ”´ Error in parameter optimization: {e}")
            raise
    
    async def _grid_search_optimization(
        self,
        objective_function: Callable,
        market_data: pd.DataFrame,
        config: OptimizationConfig
    ) -> OptimizationResult:
        """ê·¸ë¦¬ë“œ ì„œì¹˜ ìµœì í™”"""
        try:
            # ë§¤ê°œë³€ìˆ˜ ì¡°í•© ìƒì„±
            param_combinations = self._generate_parameter_grid(config.parameter_ranges)
            
            if len(param_combinations) > config.max_iterations:
                # ë„ˆë¬´ ë§ì€ ì¡°í•©ì´ë©´ ëœë¤ ìƒ˜í”Œë§
                param_combinations = random.sample(param_combinations, config.max_iterations)
            
            logger.info(f"ğŸ” Grid search: {len(param_combinations)} combinations")
            
            # ë³‘ë ¬ë¡œ ë§¤ê°œë³€ìˆ˜ ì¡°í•© í‰ê°€
            tasks = []
            for i, params in enumerate(param_combinations):
                task = self._evaluate_parameters(objective_function, market_data, params, config.objective)
                tasks.append((i, params, task))
            
            # ê²°ê³¼ ìˆ˜ì§‘
            results = []
            optimization_history = []
            best_score = float('-inf')
            
            for i, params, task in tasks:
                try:
                    score = await task
                    results.append({
                        'iteration': i,
                        'parameters': params,
                        'score': score
                    })
                    
                    if score > best_score:
                        best_score = score
                        optimization_history.append((i, best_score))
                    
                except Exception as e:
                    logger.error(f"ğŸ”´ Error evaluating parameters {i}: {e}")
            
            # ìµœê³  ê²°ê³¼ ì°¾ê¸°
            if not results:
                raise ValueError("No valid optimization results")
            
            best_result = max(results, key=lambda x: x['score'])
            
            return OptimizationResult(
                best_parameters=best_result['parameters'],
                best_score=best_result['score'],
                all_results=results,
                optimization_history=optimization_history,
                convergence_achieved=True,
                total_evaluations=len(results),
                execution_time=0,  # ë‚˜ì¤‘ì— ì„¤ì •
                method_used=config.method,
                objective_used=config.objective
            )
            
        except Exception as e:
            logger.error(f"ğŸ”´ Error in grid search optimization: {e}")
            raise
    
    async def _genetic_algorithm_optimization(
        self,
        objective_function: Callable,
        market_data: pd.DataFrame,
        config: OptimizationConfig
    ) -> OptimizationResult:
        """ìœ ì „ ì•Œê³ ë¦¬ì¦˜ ìµœì í™”"""
        try:
            # ì´ˆê¸° ê°œì²´êµ° ìƒì„±
            population = self._create_initial_population(config.parameter_ranges, config.population_size)
            
            best_score = float('-inf')
            best_parameters = None
            optimization_history = []
            all_results = []
            total_evaluations = 0
            
            logger.info(f"ğŸ§¬ Genetic algorithm: {config.population_size} population, {config.max_iterations} generations")
            
            for generation in range(config.max_iterations):
                # ê°œì²´êµ° í‰ê°€
                fitness_scores = []
                generation_results = []
                
                for individual in population:
                    try:
                        score = await self._evaluate_parameters(
                            objective_function, market_data, individual, config.objective
                        )
                        fitness_scores.append(score)
                        generation_results.append({
                            'generation': generation,
                            'parameters': individual,
                            'score': score
                        })
                        total_evaluations += 1
                        
                        # ìµœê³  ì ìˆ˜ ì—…ë°ì´íŠ¸
                        if score > best_score:
                            best_score = score
                            best_parameters = individual.copy()
                            optimization_history.append((generation, best_score))
                        
                    except Exception as e:
                        fitness_scores.append(float('-inf'))
                        logger.error(f"ğŸ”´ Error evaluating individual in generation {generation}: {e}")
                
                all_results.extend(generation_results)
                
                # ìˆ˜ë ´ í™•ì¸
                if len(optimization_history) > 10:
                    recent_improvements = [
                        optimization_history[i][1] - optimization_history[i-1][1] 
                        for i in range(-10, 0) if i < len(optimization_history)
                    ]
                    if all(imp < config.convergence_threshold for imp in recent_improvements):
                        logger.info(f"ğŸ¯ Convergence achieved at generation {generation}")
                        break
                
                # ë‹¤ìŒ ì„¸ëŒ€ ìƒì„±
                population = self._create_next_generation(
                    population, fitness_scores, config
                )
                
                # ì§„í–‰ ìƒí™© ë¡œê¹…
                avg_fitness = np.mean([f for f in fitness_scores if f != float('-inf')])
                logger.info(f"ğŸ§¬ Generation {generation}: Best {best_score:.4f}, Avg {avg_fitness:.4f}")
            
            if best_parameters is None:
                raise ValueError("No valid parameters found during optimization")
            
            return OptimizationResult(
                best_parameters=best_parameters,
                best_score=best_score,
                all_results=all_results,
                optimization_history=optimization_history,
                convergence_achieved=True,
                total_evaluations=total_evaluations,
                execution_time=0,  # ë‚˜ì¤‘ì— ì„¤ì •
                method_used=config.method,
                objective_used=config.objective
            )
            
        except Exception as e:
            logger.error(f"ğŸ”´ Error in genetic algorithm optimization: {e}")
            raise
    
    async def _bayesian_optimization(
        self,
        objective_function: Callable,
        market_data: pd.DataFrame,
        config: OptimizationConfig
    ) -> OptimizationResult:
        """ë² ì´ì§€ì•ˆ ìµœì í™”"""
        try:
            # ì´ˆê¸° ëœë¤ ìƒ˜í”Œë§
            n_initial_samples = min(20, config.max_iterations // 4)
            initial_params = self._generate_random_parameters(config.parameter_ranges, n_initial_samples)
            
            # ì´ˆê¸° í‰ê°€
            X_samples = []
            y_samples = []
            all_results = []
            
            for i, params in enumerate(initial_params):
                try:
                    score = await self._evaluate_parameters(
                        objective_function, market_data, params, config.objective
                    )
                    
                    # ë§¤ê°œë³€ìˆ˜ë¥¼ ìˆ˜ì¹˜ ë°°ì—´ë¡œ ë³€í™˜
                    param_array = self._params_to_array(params, config.parameter_ranges)
                    X_samples.append(param_array)
                    y_samples.append(score)
                    
                    all_results.append({
                        'iteration': i,
                        'parameters': params,
                        'score': score
                    })
                    
                except Exception as e:
                    logger.error(f"ğŸ”´ Error in initial sample {i}: {e}")
            
            if len(X_samples) < 5:
                raise ValueError("Insufficient initial samples for Bayesian optimization")
            
            # GP ëª¨ë¸ ì´ˆê¸°í™”
            kernel = ConstantKernel(1.0) * RBF(length_scale=1.0)
            self.gp_model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)
            
            X_array = np.array(X_samples)
            y_array = np.array(y_samples)
            
            best_score = max(y_samples)
            best_idx = y_samples.index(best_score)
            best_parameters = initial_params[best_idx]
            optimization_history = [(i, best_score) for i in range(len(initial_params))]
            
            logger.info(f"ğŸ¯ Bayesian optimization: {n_initial_samples} initial samples, best score {best_score:.4f}")
            
            # ë² ì´ì§€ì•ˆ ìµœì í™” ë°˜ë³µ
            for iteration in range(n_initial_samples, config.max_iterations):
                try:
                    # GP ëª¨ë¸ í›ˆë ¨
                    self.gp_model.fit(X_array, y_array)
                    
                    # Acquisition functionìœ¼ë¡œ ë‹¤ìŒ í¬ì¸íŠ¸ ì„ íƒ
                    next_params = self._select_next_parameters(config.parameter_ranges, X_array, y_array)
                    
                    # ìƒˆë¡œìš´ í¬ì¸íŠ¸ í‰ê°€
                    score = await self._evaluate_parameters(
                        objective_function, market_data, next_params, config.objective
                    )
                    
                    # ê²°ê³¼ ì—…ë°ì´íŠ¸
                    param_array = self._params_to_array(next_params, config.parameter_ranges)
                    X_array = np.vstack([X_array, param_array])
                    y_array = np.append(y_array, score)
                    
                    all_results.append({
                        'iteration': iteration,
                        'parameters': next_params,
                        'score': score
                    })
                    
                    if score > best_score:
                        best_score = score
                        best_parameters = next_params
                        optimization_history.append((iteration, best_score))
                    
                    # ì§„í–‰ ìƒí™© ë¡œê¹…
                    if iteration % 10 == 0:
                        logger.info(f"ğŸ¯ Bayesian iteration {iteration}: Best {best_score:.4f}")
                    
                except Exception as e:
                    logger.error(f"ğŸ”´ Error in Bayesian iteration {iteration}: {e}")
                    break
            
            return OptimizationResult(
                best_parameters=best_parameters,
                best_score=best_score,
                all_results=all_results,
                optimization_history=optimization_history,
                convergence_achieved=True,
                total_evaluations=len(all_results),
                execution_time=0,
                method_used=config.method,
                objective_used=config.objective
            )
            
        except Exception as e:
            logger.error(f"ğŸ”´ Error in Bayesian optimization: {e}")
            raise
    
    async def _random_search_optimization(
        self,
        objective_function: Callable,
        market_data: pd.DataFrame,
        config: OptimizationConfig
    ) -> OptimizationResult:
        """ëœë¤ ì„œì¹˜ ìµœì í™”"""
        try:
            # ëœë¤ ë§¤ê°œë³€ìˆ˜ ìƒì„±
            random_params = self._generate_random_parameters(config.parameter_ranges, config.max_iterations)
            
            best_score = float('-inf')
            best_parameters = None
            all_results = []
            optimization_history = []
            
            logger.info(f"ğŸ² Random search: {len(random_params)} random combinations")
            
            # ë³‘ë ¬ë¡œ í‰ê°€
            tasks = []
            for i, params in enumerate(random_params):
                task = self._evaluate_parameters(objective_function, market_data, params, config.objective)
                tasks.append((i, params, task))
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for i, params, task in tasks:
                try:
                    score = await task
                    all_results.append({
                        'iteration': i,
                        'parameters': params,
                        'score': score
                    })
                    
                    if score > best_score:
                        best_score = score
                        best_parameters = params
                        optimization_history.append((i, best_score))
                    
                except Exception as e:
                    logger.error(f"ğŸ”´ Error in random search iteration {i}: {e}")
            
            if not all_results:
                raise ValueError("No valid results from random search")
            
            return OptimizationResult(
                best_parameters=best_parameters,
                best_score=best_score,
                all_results=all_results,
                optimization_history=optimization_history,
                convergence_achieved=True,
                total_evaluations=len(all_results),
                execution_time=0,
                method_used=config.method,
                objective_used=config.objective
            )
            
        except Exception as e:
            logger.error(f"ğŸ”´ Error in random search optimization: {e}")
            raise
    
    def _generate_parameter_grid(self, parameter_ranges: Dict[str, ParameterRange]) -> List[Dict[str, Any]]:
        """ë§¤ê°œë³€ìˆ˜ ê·¸ë¦¬ë“œ ìƒì„±"""
        try:
            param_lists = {}
            
            for name, param_range in parameter_ranges.items():
                if param_range.values:
                    param_lists[name] = param_range.values
                elif param_range.step:
                    if param_range.is_integer:
                        values = list(range(int(param_range.min_value), int(param_range.max_value) + 1, int(param_range.step)))
                    else:
                        values = list(np.arange(param_range.min_value, param_range.max_value + param_range.step, param_range.step))
                    param_lists[name] = values
                else:
                    # ê¸°ë³¸ì ìœ¼ë¡œ 10ê°œ ê°’ ìƒì„±
                    if param_range.is_integer:
                        values = list(np.linspace(param_range.min_value, param_range.max_value, 10, dtype=int))
                    else:
                        values = list(np.linspace(param_range.min_value, param_range.max_value, 10))
                    param_lists[name] = values
            
            # ëª¨ë“  ì¡°í•© ìƒì„±
            keys = list(param_lists.keys())
            values = list(param_lists.values())
            
            combinations = []
            for combination in itertools.product(*values):
                param_dict = dict(zip(keys, combination))
                combinations.append(param_dict)
            
            logger.info(f"ğŸ“‹ Generated {len(combinations)} parameter combinations")
            return combinations
            
        except Exception as e:
            logger.error(f"ğŸ”´ Error generating parameter grid: {e}")
            return []
    
    def _create_initial_population(self, parameter_ranges: Dict[str, ParameterRange], population_size: int) -> List[Dict[str, Any]]:
        """ì´ˆê¸° ê°œì²´êµ° ìƒì„± (ìœ ì „ ì•Œê³ ë¦¬ì¦˜)"""
        return self._generate_random_parameters(parameter_ranges, population_size)
    
    def _generate_random_parameters(self, parameter_ranges: Dict[str, ParameterRange], count: int) -> List[Dict[str, Any]]:
        """ëœë¤ ë§¤ê°œë³€ìˆ˜ ìƒì„±"""
        try:
            random_params = []
            
            for _ in range(count):
                params = {}
                for name, param_range in parameter_ranges.items():
                    if param_range.values:
                        # ê³ ì •ëœ ê°’ ëª©ë¡ì—ì„œ ì„ íƒ
                        params[name] = random.choice(param_range.values)
                    else:
                        # ë²”ìœ„ì—ì„œ ëœë¤ ê°’ ìƒì„±
                        if param_range.is_integer:
                            params[name] = random.randint(int(param_range.min_value), int(param_range.max_value))
                        else:
                            params[name] = random.uniform(param_range.min_value, param_range.max_value)
                
                random_params.append(params)
            
            return random_params
            
        except Exception as e:
            logger.error(f"ğŸ”´ Error generating random parameters: {e}")
            return []
    
    def _create_next_generation(
        self,
        population: List[Dict[str, Any]],
        fitness_scores: List[float],
        config: OptimizationConfig
    ) -> List[Dict[str, Any]]:
        """ë‹¤ìŒ ì„¸ëŒ€ ìƒì„± (ìœ ì „ ì•Œê³ ë¦¬ì¦˜)"""
        try:
            # ìœ íš¨í•œ ê°œì²´ë“¤ë§Œ ì„ íƒ
            valid_individuals = [
                (ind, score) for ind, score in zip(population, fitness_scores)
                if score != float('-inf')
            ]
            
            if len(valid_individuals) < 2:
                # ìœ íš¨í•œ ê°œì²´ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìƒˆë¡œìš´ ëœë¤ ê°œì²´êµ° ìƒì„±
                return self._generate_random_parameters(config.parameter_ranges, config.population_size)
            
            # ì í•©ë„ ê¸°ë°˜ ì„ íƒ (ìƒìœ„ 50% ì„ íƒ)
            valid_individuals.sort(key=lambda x: x[1], reverse=True)
            elite_count = max(2, len(valid_individuals) // 2)
            elite_individuals = [ind[0] for ind in valid_individuals[:elite_count]]
            
            next_generation = []
            
            # ì—˜ë¦¬íŠ¸ ë³´ì¡´
            next_generation.extend(elite_individuals[:config.population_size // 4])
            
            # êµë°° ë° ëŒì—°ë³€ì´
            while len(next_generation) < config.population_size:
                # ë¶€ëª¨ ì„ íƒ (í† ë„ˆë¨¼íŠ¸ ì„ íƒ)
                parent1 = self._tournament_selection(valid_individuals, 3)
                parent2 = self._tournament_selection(valid_individuals, 3)
                
                # êµë°°
                if random.random() < config.crossover_rate:
                    child = self._crossover(parent1, parent2, config.parameter_ranges)
                else:
                    child = parent1.copy()
                
                # ëŒì—°ë³€ì´
                if random.random() < config.mutation_rate:
                    child = self._mutate(child, config.parameter_ranges)
                
                next_generation.append(child)
            
            return next_generation[:config.population_size]
            
        except Exception as e:
            logger.error(f"ğŸ”´ Error creating next generation: {e}")
            return self._generate_random_parameters(config.parameter_ranges, config.population_size)
    
    def _tournament_selection(self, individuals: List[Tuple[Dict, float]], tournament_size: int) -> Dict[str, Any]:
        """í† ë„ˆë¨¼íŠ¸ ì„ íƒ"""
        tournament = random.sample(individuals, min(tournament_size, len(individuals)))
        winner = max(tournament, key=lambda x: x[1])
        return winner[0]
    
    def _crossover(self, parent1: Dict[str, Any], parent2: Dict[str, Any], parameter_ranges: Dict[str, ParameterRange]) -> Dict[str, Any]:
        """êµë°°"""
        child = {}
        for param_name in parent1.keys():
            if random.random() < 0.5:
                child[param_name] = parent1[param_name]
            else:
                child[param_name] = parent2[param_name]
        return child
    
    def _mutate(self, individual: Dict[str, Any], parameter_ranges: Dict[str, ParameterRange]) -> Dict[str, Any]:
        """ëŒì—°ë³€ì´"""
        mutated = individual.copy()
        
        for param_name, param_range in parameter_ranges.items():
            if random.random() < 0.1:  # 10% í™•ë¥ ë¡œ ê° ë§¤ê°œë³€ìˆ˜ ëŒì—°ë³€ì´
                if param_range.values:
                    mutated[param_name] = random.choice(param_range.values)
                else:
                    if param_range.is_integer:
                        mutated[param_name] = random.randint(int(param_range.min_value), int(param_range.max_value))
                    else:
                        # ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€
                        current_value = individual[param_name]
                        noise_std = (param_range.max_value - param_range.min_value) * 0.1
                        new_value = current_value + np.random.normal(0, noise_std)
                        new_value = np.clip(new_value, param_range.min_value, param_range.max_value)
                        mutated[param_name] = new_value
        
        return mutated
    
    def _params_to_array(self, params: Dict[str, Any], parameter_ranges: Dict[str, ParameterRange]) -> np.ndarray:
        """ë§¤ê°œë³€ìˆ˜ë¥¼ ìˆ˜ì¹˜ ë°°ì—´ë¡œ ë³€í™˜ (ë² ì´ì§€ì•ˆ ìµœì í™”ìš©)"""
        array = []
        for name in sorted(parameter_ranges.keys()):
            value = params[name]
            param_range = parameter_ranges[name]
            
            # ì •ê·œí™” (0-1 ë²”ìœ„)
            if param_range.values:
                normalized = param_range.values.index(value) / (len(param_range.values) - 1)
            else:
                normalized = (value - param_range.min_value) / (param_range.max_value - param_range.min_value)
            
            array.append(normalized)
        
        return np.array(array)
    
    def _select_next_parameters(
        self,
        parameter_ranges: Dict[str, ParameterRange],
        X_samples: np.ndarray,
        y_samples: np.ndarray
    ) -> Dict[str, Any]:
        """ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜ ì„ íƒ (Acquisition Function)"""
        try:
            # Upper Confidence Bound (UCB) ì‚¬ìš©
            n_candidates = 100
            random_candidates = self._generate_random_parameters(parameter_ranges, n_candidates)
            
            best_acquisition = float('-inf')
            best_candidate = None
            
            for candidate in random_candidates:
                candidate_array = self._params_to_array(candidate, parameter_ranges).reshape(1, -1)
                
                # GP ì˜ˆì¸¡
                mean, std = self.gp_model.predict(candidate_array, return_std=True)
                
                # UCB acquisition function
                beta = 2.0  # íƒí—˜ vs í™œìš© ê· í˜•
                acquisition_value = mean[0] + beta * std[0]
                
                if acquisition_value > best_acquisition:
                    best_acquisition = acquisition_value
                    best_candidate = candidate
            
            return best_candidate or random_candidates[0]
            
        except Exception as e:
            logger.error(f"ğŸ”´ Error selecting next parameters: {e}")
            return self._generate_random_parameters(parameter_ranges, 1)[0]
    
    async def _evaluate_parameters(
        self,
        objective_function: Callable,
        market_data: pd.DataFrame,
        parameters: Dict[str, Any],
        objective: OptimizationObjective
    ) -> float:
        """ë§¤ê°œë³€ìˆ˜ í‰ê°€"""
        try:
            # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ë§¤ê°œë³€ìˆ˜ ì ìš©)
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” objective_functionì´ ë§¤ê°œë³€ìˆ˜ë¥¼ ë°›ì•„ì„œ ë°±í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ê²°ê³¼ ë°˜í™˜
            
            # ì‹œë®¬ë ˆì´ì…˜ëœ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼
            np.random.seed(hash(str(parameters)) % 2**32)
            
            # ë§¤ê°œë³€ìˆ˜ í’ˆì§ˆì— ë”°ë¼ ì„±ê³¼ ì¡°ì •
            param_quality = self._assess_parameter_quality(parameters)
            
            if objective == OptimizationObjective.SHARPE_RATIO:
                base_score = np.random.normal(0.5, 0.3)  # í‰ê·  0.5, í‘œì¤€í¸ì°¨ 0.3
                score = base_score * param_quality
            elif objective == OptimizationObjective.TOTAL_RETURN:
                base_score = np.random.normal(10, 5)  # í‰ê·  10%, í‘œì¤€í¸ì°¨ 5%
                score = base_score * param_quality
            elif objective == OptimizationObjective.WIN_RATE:
                base_score = np.random.uniform(0.4, 0.8)  # 40-80% ìŠ¹ë¥ 
                score = base_score * param_quality
            else:
                base_score = np.random.normal(1.5, 0.5)  # ê¸°ë³¸ ì ìˆ˜
                score = base_score * param_quality
            
            # ì ìˆ˜ í´ë¦¬í•‘
            score = max(0, score)
            
            return score
            
        except Exception as e:
            logger.error(f"ğŸ”´ Error evaluating parameters: {e}")
            return 0.0
    
    def _assess_parameter_quality(self, parameters: Dict[str, Any]) -> float:
        """ë§¤ê°œë³€ìˆ˜ í’ˆì§ˆ í‰ê°€"""
        try:
            quality_score = 1.0
            
            # ì¼ë°˜ì ì¸ ê¸°ìˆ ì  ì§€í‘œ ë§¤ê°œë³€ìˆ˜ í’ˆì§ˆ í‰ê°€
            for param_name, value in parameters.items():
                if 'period' in param_name.lower():
                    # ê¸°ê°„ ë§¤ê°œë³€ìˆ˜: ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´ í’ˆì§ˆ ì €í•˜
                    if value < 5 or value > 50:
                        quality_score *= 0.8
                    elif 10 <= value <= 30:
                        quality_score *= 1.1  # ì ì • ë²”ìœ„ëŠ” ë³´ë„ˆìŠ¤
                
                elif 'threshold' in param_name.lower():
                    # ì„ê³„ê°’ ë§¤ê°œë³€ìˆ˜: ê·¹ë‹¨ê°’ì€ í’ˆì§ˆ ì €í•˜
                    if abs(value) > 150:
                        quality_score *= 0.7
                
                elif 'multiplier' in param_name.lower():
                    # ìŠ¹ìˆ˜ ë§¤ê°œë³€ìˆ˜: 1-3 ë²”ìœ„ê°€ ì ì •
                    if not (0.5 <= value <= 5.0):
                        quality_score *= 0.8
            
            return min(1.5, max(0.3, quality_score))  # 0.3 ~ 1.5 ë²”ìœ„ë¡œ ì œí•œ
            
        except Exception as e:
            logger.error(f"ğŸ”´ Error assessing parameter quality: {e}")
            return 1.0
    
    async def auto_optimize_strategy(
        self,
        strategy_name: str,
        performance_data: List[Dict],
        market_data: pd.DataFrame
    ) -> Dict[str, Any]:
        """ì „ëµ ìë™ ìµœì í™”"""
        try:
            # ì „ëµë³„ ê¸°ë³¸ ë§¤ê°œë³€ìˆ˜ ë²”ìœ„ ì„¤ì •
            if 'cci' in strategy_name.lower():
                parameter_ranges = {
                    'cci_period': ParameterRange('cci_period', 10, 30, is_integer=True),
                    'overbought': ParameterRange('overbought', 80, 150),
                    'oversold': ParameterRange('oversold', -150, -80)
                }
            elif 'rsi' in strategy_name.lower():
                parameter_ranges = {
                    'rsi_period': ParameterRange('rsi_period', 10, 25, is_integer=True),
                    'overbought': ParameterRange('overbought', 65, 85),
                    'oversold': ParameterRange('oversold', 15, 35)
                }
            elif 'macd' in strategy_name.lower():
                parameter_ranges = {
                    'fast_period': ParameterRange('fast_period', 8, 16, is_integer=True),
                    'slow_period': ParameterRange('slow_period', 20, 35, is_integer=True),
                    'signal_period': ParameterRange('signal_period', 7, 12, is_integer=True)
                }
            else:
                # ê¸°ë³¸ ë§¤ê°œë³€ìˆ˜
                parameter_ranges = {
                    'period': ParameterRange('period', 10, 30, is_integer=True),
                    'threshold': ParameterRange('threshold', 0.5, 2.0)
                }
            
            # ìµœì í™” ì„¤ì •
            config = OptimizationConfig(
                method=OptimizationMethod.GENETIC_ALGORITHM,
                objective=OptimizationObjective.SHARPE_RATIO,
                parameter_ranges=parameter_ranges,
                max_iterations=30,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¶•ì†Œ
                population_size=20
            )
            
            # ê°„ë‹¨í•œ ëª©ì  í•¨ìˆ˜ (ì‹¤ì œë¡œëŠ” ì „ëµë³„ ë°±í…ŒìŠ¤íŠ¸ í•¨ìˆ˜)
            async def simple_objective(data):
                return np.random.normal(0.5, 0.2)  # ì‹œë®¬ë ˆì´ì…˜
            
            # ìµœì í™” ì‹¤í–‰ (ì´ë¯¸ async ì»¨í…ìŠ¤íŠ¸ ë‚´ë¶€ì´ë¯€ë¡œ await ì‚¬ìš©)
            result = await self.optimize_parameters(simple_objective, config, market_data)
            
            # ê¸°ì¡´ ë§¤ê°œë³€ìˆ˜ì™€ ë¹„êµ
            current_performance = self._calculate_current_performance(performance_data)
            improvement = result.best_score - current_performance
            
            return {
                'strategy_name': strategy_name,
                'optimization_result': {
                    'best_parameters': result.best_parameters,
                    'best_score': result.best_score,
                    'improvement': improvement,
                    'improvement_pct': (improvement / current_performance * 100) if current_performance > 0 else 0,
                    'evaluations': result.total_evaluations,
                    'execution_time': result.execution_time
                },
                'current_performance': current_performance,
                'recommendation': 'APPLY' if improvement > 0.1 else 'KEEP_CURRENT',
                'confidence': min(1.0, abs(improvement) / 0.5),  # ê°œì„ ì´ í´ìˆ˜ë¡ ë†’ì€ ì‹ ë¢°ë„
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ğŸ”´ Error in auto strategy optimization: {e}")
            return {'error': str(e)}
    
    def _calculate_current_performance(self, performance_data: List[Dict]) -> float:
        """í˜„ì¬ ì„±ê³¼ ê³„ì‚°"""
        try:
            if not performance_data:
                return 0.0
            
            returns = [trade.get('pnl_pct', 0) / 100 for trade in performance_data]
            
            if len(returns) < 2:
                return 0.0
            
            # Sharpe Ratio ê³„ì‚°
            mean_return = np.mean(returns)
            std_return = np.std(returns)
            
            sharpe = mean_return / std_return * np.sqrt(252) if std_return > 0 else 0
            
            return sharpe
            
        except Exception as e:
            logger.error(f"ğŸ”´ Error calculating current performance: {e}")
            return 0.0
    
    def get_optimization_summary(self) -> Dict[str, Any]:
        """ìµœì í™” ìš”ì•½"""
        try:
            if not self.optimization_history:
                return {'message': 'No optimization history available'}
            
            recent_optimizations = self.optimization_history[-10:]
            
            # ë°©ë²•ë³„ ì„±ê³¼
            method_performance = {}
            for result in recent_optimizations:
                method = result.method_used.value
                if method not in method_performance:
                    method_performance[method] = {'count': 0, 'avg_score': 0, 'best_score': 0}
                
                method_performance[method]['count'] += 1
                method_performance[method]['avg_score'] += result.best_score
                method_performance[method]['best_score'] = max(method_performance[method]['best_score'], result.best_score)
            
            for method, perf in method_performance.items():
                perf['avg_score'] /= perf['count']
                perf['avg_score'] = round(perf['avg_score'], 4)
                perf['best_score'] = round(perf['best_score'], 4)
            
            # ìµœê³  ê²°ê³¼
            best_result = max(recent_optimizations, key=lambda x: x.best_score)
            
            return {
                'total_optimizations': len(self.optimization_history),
                'recent_optimizations': len(recent_optimizations),
                'method_performance': method_performance,
                'best_result': {
                    'score': round(best_result.best_score, 4),
                    'method': best_result.method_used.value,
                    'objective': best_result.objective_used.value,
                    'parameters': best_result.best_parameters,
                    'timestamp': best_result.timestamp.isoformat()
                },
                'optimization_efficiency': {
                    'avg_evaluations': round(np.mean([r.total_evaluations for r in recent_optimizations]), 1),
                    'avg_execution_time': round(np.mean([r.execution_time for r in recent_optimizations]), 2),
                    'convergence_rate': sum(1 for r in recent_optimizations if r.convergence_achieved) / len(recent_optimizations) * 100
                }
            }
            
        except Exception as e:
            logger.error(f"ğŸ”´ Error getting optimization summary: {e}")
            return {'error': str(e)}

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_parameter_optimizer():
    """Parameter Optimizer í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Testing Parameter Optimizer...")
    
    # ìµœì í™”ê¸° ìƒì„±
    optimizer = ParameterOptimizer(max_workers=2)
    
    # ìƒ˜í”Œ ì‹œì¥ ë°ì´í„°
    dates = pd.date_range('2025-01-01', periods=1000, freq='5min')
    np.random.seed(42)
    
    price_base = 50000
    price_data = price_base + np.cumsum(np.random.randn(1000) * 50)
    
    market_data = pd.DataFrame({
        'timestamp': dates,
        'open': price_data + np.random.randn(1000) * 25,
        'high': price_data + np.abs(np.random.randn(1000) * 50),
        'low': price_data - np.abs(np.random.randn(1000) * 50),
        'close': price_data,
        'volume': np.random.randint(1000, 10000, 1000)
    })
    
    # ê°„ë‹¨í•œ ëª©ì  í•¨ìˆ˜
    async def test_objective(data):
        await asyncio.sleep(0.01)  # ì‹œë®¬ë ˆì´ì…˜ ì§€ì—°
        return np.random.normal(1.0, 0.3)
    
    # ë§¤ê°œë³€ìˆ˜ ë²”ìœ„ ì •ì˜
    parameter_ranges = {
        'cci_period': ParameterRange('cci_period', 10, 30, is_integer=True),
        'overbought': ParameterRange('overbought', 80, 120),
        'oversold': ParameterRange('oversold', -120, -80)
    }
    
    # 1. ê·¸ë¦¬ë“œ ì„œì¹˜ í…ŒìŠ¤íŠ¸
    print("ğŸ” Grid Search Optimization:")
    grid_config = OptimizationConfig(
        method=OptimizationMethod.GRID_SEARCH,
        objective=OptimizationObjective.SHARPE_RATIO,
        parameter_ranges=parameter_ranges,
        max_iterations=20
    )
    
    try:
        grid_result = await optimizer.optimize_parameters(test_objective, grid_config, market_data)
        print(f"  - Best Score: {grid_result.best_score:.4f}")
        print(f"  - Best Parameters: {grid_result.best_parameters}")
        print(f"  - Evaluations: {grid_result.total_evaluations}")
        print(f"  - Time: {grid_result.execution_time:.2f}s")
    except Exception as e:
        print(f"  - Grid Search Error: {e}")
    
    # 2. ìœ ì „ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸
    print(f"\\nğŸ§¬ Genetic Algorithm Optimization:")
    ga_config = OptimizationConfig(
        method=OptimizationMethod.GENETIC_ALGORITHM,
        objective=OptimizationObjective.SHARPE_RATIO,
        parameter_ranges=parameter_ranges,
        max_iterations=10,
        population_size=15
    )
    
    try:
        ga_result = await optimizer.optimize_parameters(test_objective, ga_config, market_data)
        print(f"  - Best Score: {ga_result.best_score:.4f}")
        print(f"  - Best Parameters: {ga_result.best_parameters}")
        print(f"  - Generations: {len(ga_result.optimization_history)}")
        print(f"  - Convergence: {ga_result.convergence_achieved}")
    except Exception as e:
        print(f"  - Genetic Algorithm Error: {e}")
    
    # 3. ëœë¤ ì„œì¹˜ í…ŒìŠ¤íŠ¸
    print(f"\\nğŸ² Random Search Optimization:")
    random_config = OptimizationConfig(
        method=OptimizationMethod.RANDOM_SEARCH,
        objective=OptimizationObjective.TOTAL_RETURN,
        parameter_ranges=parameter_ranges,
        max_iterations=25
    )
    
    try:
        random_result = await optimizer.optimize_parameters(test_objective, random_config, market_data)
        print(f"  - Best Score: {random_result.best_score:.4f}")
        print(f"  - Best Parameters: {random_result.best_parameters}")
        print(f"  - Evaluations: {random_result.total_evaluations}")
    except Exception as e:
        print(f"  - Random Search Error: {e}")
    
    # 4. ìë™ ì „ëµ ìµœì í™” í…ŒìŠ¤íŠ¸
    print(f"\\nğŸ¯ Auto Strategy Optimization:")
    sample_performance = [
        {'pnl': 150, 'pnl_pct': 3.0, 'timestamp': datetime.now() - timedelta(days=i)}
        for i in range(20)
    ]
    
    try:
        auto_result = optimizer.auto_optimize_strategy("CCI_Strategy", sample_performance, market_data)
        print(f"  - Strategy: {auto_result['strategy_name']}")
        print(f"  - Best Score: {auto_result['optimization_result']['best_score']:.4f}")
        print(f"  - Improvement: {auto_result['optimization_result']['improvement']:.4f}")
        print(f"  - Recommendation: {auto_result['recommendation']}")
        print(f"  - Confidence: {auto_result['confidence']:.2f}")
    except Exception as e:
        print(f"  - Auto Optimization Error: {e}")
    
    # 5. ìµœì í™” ìš”ì•½
    print(f"\\nğŸ“Š Optimization Summary:")
    summary = optimizer.get_optimization_summary()
    if 'error' not in summary:
        print(f"  - Total optimizations: {summary['total_optimizations']}")
        print(f"  - Method performance: {summary['method_performance']}")
        print(f"  - Convergence rate: {summary['optimization_efficiency']['convergence_rate']:.1f}%")
    else:
        print(f"  - Summary Error: {summary['error']}")

if __name__ == "__main__":
    import asyncio
    import time
    asyncio.run(test_parameter_optimizer())